{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAQuKS774j0FTFnd5/IWjA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Haadhi-Mohammed/Projects/blob/main/Customer_Segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RFX7llRU0Fs",
        "outputId": "9be019db-3015-48ad-e30f-508f3c6115ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488490 sha256=e89ee1578f39b0e13be8f7f07f97e0c5f9f59274e4f15f1a857ee24ea5cb91d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "# installing PySpark\n",
        "\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local[*]\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4040')\\\n",
        "        .getOrCreate()\n",
        "print(spark)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yKcr27KU4Um",
        "outputId": "418d318b-e263-4d79-b3a3-f947f0d2ba1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x7f55e023a5f0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing Necessary Packages\n",
        "\n",
        "from pyspark.sql.functions import col, isnan, when, count\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler, PCA\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "from pyspark.ml.feature import PCA\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "\n",
        "from pyspark.sql.functions import udf, col\n",
        "from pyspark.sql.types import ArrayType, DoubleType\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "EDihu7oVU7To"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "df = spark.read.csv(\"/content/Cleaned_CusSeg.csv\", header=True, inferSchema=True)\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXcEAnitWMlx",
        "outputId": "f2859976-3ded-4292-bdf8-8da96c3bb607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+----------+--------------+------+-------+--------+-----------+-------+-----+------+------------+------------+-------------+------------+-----------------+---------------+-------------------+-----------------+-----------------+------------+------------+------------+------------+------------+--------+--------+\n",
            "|  ID|Age| Education|Marital_Status|Income|Kidhome|Teenhome|Dt_Customer|Recency|Wines|Fruits|MeatProducts|FishProducts|SweetProducts|GoldProducts|NumDealsPurchases|NumWebPurchases|NumCatalogPurchases|NumStorePurchases|NumWebVisitsMonth|AcceptedCmp3|AcceptedCmp4|AcceptedCmp5|AcceptedCmp1|AcceptedCmp2|Complain|Response|\n",
            "+----+---+----------+--------------+------+-------+--------+-----------+-------+-----+------+------------+------------+-------------+------------+-----------------+---------------+-------------------+-----------------+-----------------+------------+------------+------------+------------+------------+--------+--------+\n",
            "|5524| 57|Graduation|        Single| 58138|      0|       0|       2012|     58|  635|    88|         546|         172|           88|          88|                3|              8|                 10|                4|                7|           0|           0|           0|           0|           0|       0|       1|\n",
            "|2174| 60|Graduation|        Single| 46344|      1|       1|       2014|     38|   11|     1|           6|           2|            1|           6|                2|              1|                  1|                2|                5|           0|           0|           0|           0|           0|       0|       0|\n",
            "|4141| 49|Graduation|      Together| 71613|      0|       0|       2013|     26|  426|    49|         127|         111|           21|          42|                1|              8|                  2|               10|                4|           0|           0|           0|           0|           0|       0|       0|\n",
            "|6182| 30|Graduation|      Together| 26646|      1|       0|       2014|     26|   11|     4|          20|          10|            3|           5|                2|              2|                  0|                4|                6|           0|           0|           0|           0|           0|       0|       0|\n",
            "|5324| 33|       PhD|       Married| 58293|      1|       0|       2014|     94|  173|    43|         118|          46|           27|          15|                5|              5|                  3|                6|                5|           0|           0|           0|           0|           0|       0|       0|\n",
            "+----+---+----------+--------------+------+-------+--------+-----------+-------+-----+------+------------+------------+-------------+------------+-----------------+---------------+-------------------+-----------------+-----------------+------------+------------+------------+------------+------------+--------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting categorical columns to numerical using StringIndexer and OneHotEncoder\n",
        "categorical_columns = ['Education', 'Marital_Status']\n",
        "indexer = [StringIndexer(inputCol=col, outputCol=col + \"_index\") for col in categorical_columns]\n",
        "encoder = [OneHotEncoder(inputCols=[col + \"_index\"], outputCols=[col + \"_vec\"]) for col in categorical_columns]"
      ],
      "metadata": {
        "id": "Ui2UGV5fWU6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Numeric feature columns\n",
        "numeric_columns = ['Age', 'Income', 'Kidhome', 'Teenhome', 'Dt_Customer', 'Recency',\n",
        "                   'Wines', 'Fruits', 'MeatProducts', 'FishProducts', 'SweetProducts',\n",
        "                   'GoldProducts', 'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases',\n",
        "                   'NumStorePurchases', 'NumWebVisitsMonth', 'AcceptedCmp3', 'AcceptedCmp4',\n",
        "                   'AcceptedCmp5', 'AcceptedCmp1', 'AcceptedCmp2', 'Complain', 'Response']"
      ],
      "metadata": {
        "id": "xDkVzfBsWUzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assembling features into a single vector\n",
        "assembler = VectorAssembler(inputCols=[i + \"_vec\" for i in categorical_columns] + numeric_columns,\n",
        "                            outputCol=\"features\")\n",
        "\n",
        "# Standardizing the features\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")"
      ],
      "metadata": {
        "id": "S5KeCwqFWfTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pipeline\n",
        "pipeline = Pipeline(stages=indexer + encoder + [assembler, scaler])\n",
        "\n",
        "# Fit and transform the data\n",
        "model = pipeline.fit(df)\n",
        "df_transformed = model.transform(df)"
      ],
      "metadata": {
        "id": "3PJOCzBcW8Ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA\n",
        "pca = PCA(k=len(numeric_columns) + len(categorical_columns), inputCol=\"scaled_features\", outputCol=\"pca_features\")\n",
        "pca_model = pca.fit(df_transformed)\n",
        "\n",
        "# Explained variance\n",
        "explained_variance = pca_model.explainedVariance.toArray()\n",
        "\n",
        "# Cumulative explained variance\n",
        "cumulative_variance = np.cumsum(explained_variance)\n",
        "\n",
        "# Number of components to keep to explain 90% of the variance\n",
        "threshold = 0.90\n",
        "num_components = np.where(cumulative_variance >= threshold)[0][0] + 1\n",
        "\n",
        "print('Number of components to retain',threshold*100,'% variance:',num_components)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssXufC0JXI1h",
        "outputId": "45460be3-784e-4b18-b5f0-dbb5dd0457a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of components to retain 90.0 % variance: 21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply PCA with 21 components\n",
        "pca = PCA(k=21, inputCol=\"scaled_features\", outputCol=\"pca_features\")\n",
        "pca_model = pca.fit(df_transformed)\n",
        "df_pca = pca_model.transform(df_transformed)"
      ],
      "metadata": {
        "id": "ImkEy44_XboC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Silhouette Score Calculation\n",
        "for k in range(2, 10):\n",
        "    kmeans = KMeans(k=k, seed=1, featuresCol=\"pca_features\", predictionCol=\"cluster\")\n",
        "    kmeans_model = kmeans.fit(df_pca)\n",
        "    df_clusters = kmeans_model.transform(df_pca)\n",
        "    evaluator = ClusteringEvaluator(predictionCol='cluster', featuresCol='pca_features', metricName='silhouette')\n",
        "    silhouette_score = evaluator.evaluate(df_clusters)\n",
        "    print('Silhouette Score for k=',k,':',silhouette_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAB_WKABYEnm",
        "outputId": "e11007fe-8b26-42c5-cc27-8b50cfc63365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette Score for k= 2 : 0.3292707148738423\n",
            "Silhouette Score for k= 3 : 0.33916962913549553\n",
            "Silhouette Score for k= 4 : 0.22228086143399467\n",
            "Silhouette Score for k= 5 : 0.23437840352882616\n",
            "Silhouette Score for k= 6 : 0.24289909487283445\n",
            "Silhouette Score for k= 7 : 0.15771609718939686\n",
            "Silhouette Score for k= 8 : 0.15929187270698264\n",
            "Silhouette Score for k= 9 : 0.16149693063315124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# K-Means Clustering with k=3\n",
        "k = 3\n",
        "kmeans = KMeans(k=k, seed=1, featuresCol=\"pca_features\", predictionCol=\"cluster\")\n",
        "kmeans_model = kmeans.fit(df_pca)\n",
        "df_clusters = kmeans_model.transform(df_pca)\n",
        "\n",
        "# Show the results\n",
        "df_clusters.select(\"Id\", \"cluster\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lff1q2J2XfD8",
        "outputId": "2897b668-900d-44d4-f07a-7b84cc6a9e43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------+\n",
            "|Id  |cluster|\n",
            "+----+-------+\n",
            "|5524|0      |\n",
            "|2174|1      |\n",
            "|4141|0      |\n",
            "|6182|1      |\n",
            "|5324|1      |\n",
            "|7446|0      |\n",
            "|965 |0      |\n",
            "|6177|1      |\n",
            "|4855|1      |\n",
            "|5899|1      |\n",
            "|387 |1      |\n",
            "|2125|0      |\n",
            "|8180|1      |\n",
            "|2569|1      |\n",
            "|2114|0      |\n",
            "|9736|1      |\n",
            "|4939|1      |\n",
            "|6565|0      |\n",
            "|2278|1      |\n",
            "|9360|1      |\n",
            "+----+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_clusters.groupBy('cluster').count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXzSrfMBY-k2",
        "outputId": "b0a021e5-f0c7-454d-f256-60cd454ea1cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|cluster|count|\n",
            "+-------+-----+\n",
            "|      1| 1317|\n",
            "|      2|   20|\n",
            "|      0|  867|\n",
            "+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract components from the Vector\n",
        "def extract_pca_components(v):\n",
        "    return v.toArray().tolist()\n",
        "\n",
        "extract_pca_components_udf = udf(extract_pca_components, ArrayType(DoubleType()))\n",
        "\n",
        "# create an array column\n",
        "df_clusters = df_clusters.withColumn(\"pca_array\", extract_pca_components_udf(col(\"pca_features\")))\n",
        "\n",
        "# Split the array column into separate columns\n",
        "for i in range(num_components):\n",
        "    df_clusters = df_clusters.withColumn(f\"PC{i+1}\", col(\"pca_array\")[i])\n",
        "\n",
        "# Drop the temporary array column\n",
        "df_clusters = df_clusters.drop(\"pca_array\")"
      ],
      "metadata": {
        "id": "u1maDOddZKG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of columns\n",
        "columns = [\n",
        "    'ID', 'Age', 'Education', 'Marital_Status', 'Income', 'Kidhome', 'Teenhome',\n",
        "    'Dt_Customer', 'Recency', 'Wines', 'Fruits', 'MeatProducts', 'FishProducts',\n",
        "    'SweetProducts', 'GoldProducts', 'NumDealsPurchases', 'NumWebPurchases',\n",
        "    'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth', 'AcceptedCmp3',\n",
        "    'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1', 'AcceptedCmp2', 'Complain',\n",
        "    'Response', 'cluster', 'PC1', 'PC2', 'PC3']\n",
        "\n",
        "df_final = df_clusters.select(columns)"
      ],
      "metadata": {
        "id": "SUJ-ah3SZigw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the DataFrame to CSV\n",
        "df_final.toPandas().to_csv('Clustered_Data.csv', index=False)"
      ],
      "metadata": {
        "id": "hTVXcAdXZyf8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}